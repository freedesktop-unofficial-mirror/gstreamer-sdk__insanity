#!/usr/bin/env python

# GStreamer QA system
#
#       gst-media-test
#
# Copyright (c) 2007, Edward Hervey <bilboed@bilboed.com>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this program; if not, write to the
# Free Software Foundation, Inc., 59 Temple Place - Suite 330,
# Boston, MA 02111-1307, USA.

"""
Attempts to simulate the behaviour of the (now deprecated) gst-media-test
"""

# TODO
#
# Create a scenario from the given inputs
#
# Find similarities with other CLI client and move those up in a
# CLI base class.

import sys
import string
import time
import os.path
from optparse import OptionParser
from gstqa.client import TesterClient
from gstqa.scenario import Scenario, ListScenario
from gstqa.generator import FileSystemGenerator, URIFileSystemGenerator
from gstqa.monitor import GstDebugLogMonitor, ValgrindMemCheckMonitor, GDBMonitor
from gstqa.test import PythonDBusTest
from gstqa.testrun import TestRun, ListTestRun
import gstqa.utils as utils
from tests.playbin import PlaybinTest
from tests.ismedia import IsMediaTest
from tests.scenarios.mediascenario import MediaBarrierScenario

class GstMediaTestScenario(Scenario):
    """
    This is a scenario that will attempt to run the given test.
    If it doesn't reach 100% succes, it will be re-run with more aggressive
    monitoring.

    It automatically adds the correct monitors to the underlying tests, or
    sets the right parameter for tests that have default monitor.

    This reproduces the re-try behaviour of gst-media-test
    """
    __test_name__ = "GstMediaTestScenario"
    __test_arguments__ = {
        "subtest-class":"TestClass to run",
        "debug-level-1":"GST_DEBUG level to use on first run (default:'*:2')",
        "debug-level-2":"GST_DEBUG level to use on second run (default:'*:5')"
        }
    __test_checklist__ = {
        "similar-results":"were the results similar over the two runs"
        }

    def setUp(self):
        if not Scenario.setUp(self):
            return False
        # add the initial test
        subtest = self.arguments.get("subtest-class")
        debuglevel = self.arguments.get("debug-level-1", "*:2")
        if not subtest:
            return False
        self.addSubTest(subtest, self.arguments,
                        [(GstDebugLogMonitor, {"debug-level": debuglevel})
                         ])
        return True

    def subTestDone(self, test):
        if len(self.tests) == 2:
            if test.getSuccessPercentage() == self.tests[0].getSuccessPercentage():
                self.validateStep("similar-results")
            return True
        if not test.getSuccessPercentage() == 100.0:
            subtest = self.arguments.get("subtest-class")
            debuglevel = self.arguments.get("debug-level-2", "*:5")
            self.addSubTest(subtest, self.arguments,
                            [(GstDebugLogMonitor, {"debug-level": debuglevel})
                             ])
        else:
            self.validateStep("similar-results")
        return True


class GstMediaTestClient(TesterClient):

    __software_name__ = """gst-media-test"""
    # simulate the behaviour of the previous gst-media-test
    #
    # Here we only run for one TestRun and then exit
    #
    # Some options might be difficult to reproduce with the new system.

    def __init__(self, testrun, verbose=False):
        TesterClient.__init__(self, singlerun=True)
        self.addTestRun(testrun)
        self._verbose = verbose

    def test_run_start(self, testrun):
        print "Starting", testrun
        testrun.connect("single-test-done", self._singleTestDoneCb)
        if self._verbose:
            self._printTestRunEnvironment(testrun)

    def test_run_done(self, testrun):
        print "Done with", testrun
        ids = self._storage.listTestRuns()
        for key in ids:
            clientid, starttime, stoptime = self._storage.getTestRun(key)
            print "TestRun #%d from client %d" % (key, clientid)
            softw,name,user = self._storage.getClientInfoForTestRun(key)
            print "\t\tsoftware:%s,name:%s,user:%s" % (softw,name,user)
            print "\tStart:", time.ctime(starttime)
            print "\tStop:", time.ctime(stoptime)
            print "\tNb tests:", len(self._storage.getTestsForTestRun(key))
            print "\n"

    def test_run_aborted(self, testrun):
        print "Aborted", testrun

    def _singleTestDoneCb(self, testrun, test):
        self.printSingleTestResult(test)

    def _printTestRunEnvironment(self, testrun):
        d = testrun.getEnvironment()
        if d:
            print "Environment:"
            for key,val in d.iteritems():
                if isinstance(val, dict):
                    print "\t% -30s:" % (key)
                    for dk,dv in val.iteritems():
                        print "\t\t% -30s:\t%s" % (dk,dv)
                else:
                    print "\t% -30s:\t%s" % (key,val)

    def printSingleTestResult(self, test, offset=0):
        stub = " " * offset
        print stub, "Test %r is done (Success:%0.1f%%)" % (test, test.getSuccessPercentage())
        if self._verbose:
            # print out all details from test
            print stub, "Arguments:"
            ta = test.arguments
            fa = test.getFullArgumentList()
            for arg in [x for x in fa if ta.has_key(x)]:
                print stub, "  %s : %s\t\t%s" % (arg, fa[arg], ta[arg])
            # print results from test
            print stub, "Results"
            tc = test.getCheckList()
            fc = test.getFullCheckList()
            for c in fc:
                print stub, "  %30s:%10s\t\t%s" % (c, tc[c], fc[c])

            infos = test.getExtraInfo()
            if infos:
                print stub, "Extra information:"
            for extra in infos:
                print stub, "  %30s :\t%s" % (extra, infos[extra])
        if isinstance(test, Scenario):
            for sub in test.tests:
                print stub, "Sub Test"
                self.printSingleTestResult(sub, offset=offset+4)
            print "\n"

def create_testrun(files, testscripts, recursive=True, topdir=None,
                   extreme=False, valgrinding=False,
                   debuglevel=2, debuglevel2=5,
                   acceptlist=[], rejectlist=[],
                   compressed=True, subdirectory=None, weboutput=True,
                   maxnbtests=2):
    """
    Takes the parameters of gst-media-test and creates a TestRun object
    """
    # the testrun will just be a normal list
    # Each test will be a Scenario that will rerun the test with more
    # aggressive settings
    # Use generator for the arguments
    # TODO : port existing gst-media-test tests to new classes

    # FileSystemGenerator from the following arguments:
    # * files
    # * recursive
    # * acceptlist
    # * rejectlist
    generator1 = URIFileSystemGenerator(paths=files, recursive=recursive,
                                        matching=acceptlist,
                                        reject=rejectlist)

    # get the classes corresponding to the given tests
    tests = [utils.get_test_class(name) for name in testscripts]

    # our monitors
    gdbscriptfile = os.path.join(os.path.dirname(os.path.abspath(__file__)), "gdb.instructions")
    monitors = [(GDBMonitor, {"gdb-script" : gdbscriptfile})]
    if valgrinding:
        # get full location of gst.supp
        suppfile = os.path.join(os.path.dirname(os.path.abspath(__file__)), "gst.supp")
        monitors.append((ValgrindMemCheckMonitor,
                         {"suppression-files":suppfile}))

    testrun = TestRun(maxnbtests=maxnbtests)
    for test in tests:
        testrun.addTest(GstMediaTestScenario,
                        arguments = { "uri" : generator1,
                                      "subtest-class": test,
                                      "debug-level-1":str(debuglevel),
                                      "debug-level-2":str(debuglevel2) },
                        monitors = monitors)

#     testrun = ListTestRun(tests,
#                           arguments = { "uri" : generator1 },
#                           maxnbtests=maxnbtests,
#                           monitors = monitors)

    return testrun

def fileparse_callback(option, opt, value, parser):
    assert value is None
    value = []
    rargs = parser.rargs
    while rargs:
        arg = rargs[0]
        if ((arg[:2] == "--" and len(arg) > 2) or
            (arg[:1] == "-" and len(arg) > 1 and arg[1] != "-")):
            break
        else:
            value.append(arg)
            del rargs[0]

    setattr(parser.values, option.dest, value)

if __name__ == "__main__":
    """ Run the given files with the given testscript """
    # We should have arguments for choosing
    # _ A top output directory (topdir)
    # _ The list of tests to run
    parser = OptionParser()
    parser.add_option("-t", "--tests", dest="tests",
                      action="callback", callback=fileparse_callback,
                      help="test(s) to run on the given files",
                      default=[])
    parser.add_option("-f", "--files", dest="files",
                      help="list of file and/or directories to test",
                      action="callback", callback=fileparse_callback,
                      default=[])
    parser.add_option("-o", "--output", dest="output",
                      help="top-level output directory (default: current)", metavar="DIRECTORY",
                      default=None)
    parser.add_option("-v", "--valgrind", dest="valgrind",
                     help="Run within valgrind",
                     action="store_true",
                     default=False)
    parser.add_option("-V", "--verbose", dest="verbose",
                     help="Verbose output",
                     action="store_true",
                     default=False)
    parser.add_option("-n", "--nonrecursive", dest="recursive",
                      help="Don't go recursively in directories",
                      action="store_false",
                      default=True)
    parser.add_option("-x", "--extreme", dest="extreme",
                      help="Valgrind failed test (even without -v)",
                      action="store_true",
                      default=False)
    parser.add_option("-d", "--debuglevel", dest="debuglevel", type="int",
                      default=2, help="GStreamer debug level (default: 2)")
    parser.add_option("-D", "--debuglevel2", dest="debuglevel2", type="int",
                      default=5, help="GStreamer debug level for 2nd run (default: 5)")
    parser.add_option("-a", "--accept", dest="accept",
                      help="Coma-separated list of file extensions to limit the tests to.",
                      default=None, metavar="EXTENSIONS")
    parser.add_option("-e", "--exclude", dest="exclude",
                      help="Coma-separated list of file extensions to exclude.",
                      default=None, metavar="EXTENSIONS")
    parser.add_option("-u", "--uncompressed-logs", dest="compressed",
                      action="store_false", default=True,
                      help="Don't compress debug logs")
    parser.add_option("-s", "--subdirectory-name", dest="subdirectory",
                      help="name of output sub-directory (default date)",
                      default=None)
    parser.add_option("-S", "--simultaneous", dest="maxnbtests",
                      type="int", default=1, help="Maximum number of simultaneous tests (default:1)")
    parser.add_option("-w", "--no-output", dest="weboutput",
                      action="store_false", default=True,
                      help="Do not automatically generate html outputs")
    (options, args) = parser.parse_args(sys.argv[1:])
    files = options.files
    tests = options.tests
    if options.accept:
        acceptlist = options.accept.split(",")
    else:
        acceptlist = []
    if options.exclude:
        rejectlist = options.exclude.split(",")
    else:
        rejectlist = []
    if not len(files) and not len(tests):
        parser.print_help()
        tests = utils.list_available_tests()
        print "Available tests:"
        for name, desc, cls in tests:
            print "\t% -20s : %s" % (name, desc)
        sys.exit()
    if not len(files):
        print "No files/directories to test were given"
    elif not len(tests):
        print "No test was given"
    elif not options.output == None and not os.path.isdir(options.output):
        print "%s is not a valid directory" % options.output
    else:
        testrun = create_testrun(files, tests,
                                 topdir=options.output,
                                 recursive=options.recursive,
                                 extreme=options.extreme,
                                 valgrinding=options.valgrind,
                                 debuglevel=options.debuglevel,
                                 debuglevel2=options.debuglevel2,
                                 acceptlist=acceptlist,
                                 rejectlist=rejectlist,
                                 compressed=options.compressed,
                                 subdirectory=options.subdirectory,
                                 weboutput=options.weboutput,
                                 maxnbtests=options.maxnbtests)
        tester = GstMediaTestClient(testrun, verbose=options.verbose)
        tester.run()
